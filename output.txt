Question 1: 
Number of word types in training corpus, including padding symbols and the unknown token: 	15031

Question 2: 
Total number of word tokens in training corpus: 	498474

Question 3:
Percentage of word types in Brown test corpus that did not occur in training:	22.759315206445116%
Percentage of word tokens in Brown test corpus that did not occur in training:	5.994167836699427%
Percentage of word types in Learner test corpus that did not occur in training:	16.348195329087048%
Percentage of word tokens in Learner test corpus that did not occur in training:	5.04907306434024%

Question 4:
Percentage of bigram types in Brown test corpus that did not occur in training:	38.513919487282195%
Percentage of bigram tokens in Brown test corpus that did not occur in training:	21.64497488794081%
Percentage of bigram types in Learner test corpus that did not occur in training:	38.63540346430081%
Percentage of bigram tokens in Learner test corpus that did not occur in training:	23.492201984949286%

Questions 5 and 6:

For the sentence "He was laughed off the screen . ": 

In the Unigram Model: 
The sentence gets mapped to ['he', 'was', 'laughed', 'off', 'the', 'screen', '.', '</s>']
The parameters required to compute the probabilities are:
"he" : 0.012608101186520317
"was" : 0.010897954173139686
"laughed" : 9.101029897941474e-05
"off" : 0.000850840469528482
"the" : 0.05218699864966114
"screen" : 3.1747778713749326e-05
"." : 0.047067140202423835
"</s>" : 0.05502948310383217

The log base 2 of each of these probabilities is:
"he" : -6.309505171448915
"was" : -6.519798860395025
"laughed" : -13.423610660490384
"off" : -10.198823724013554
"the" : -4.2601657569245654
"screen" : -14.942984819583963
"." : -4.409135992178209
"</s>" : -4.1836514123893025

The sum of the log probabilities (and the log probability of this text) is:	-64.24767639742392
The average log probability for this text is:	-8.03095954967799
The perplexity for this text is:	261.55300764185296

In the Bigram MLE Model: 
The sentence gets mapped to ['<s>', 'he', 'was', 'laughed', 'off', 'the', 'screen', '.', '</s>']
The following parameters need to be computed:
"laughed off" : 0.0
". </s>" : 1.0
"he was" : 0.11616585529629007
"off the" : 0.1865671641791045
"<s> he" : 0.08203846153846153
"the screen" : 0.00012166930283489475
"was laughed" : 0.0
"screen ." : 0.0

The log probability is undefined due to the following unseen bigrams:
"laughed off"
"was laughed"
"screen ."

In the Bigram Add-One Model: 
The sentence gets mapped to ['<s>', 'he', 'was', 'laughed', 'off', 'the', 'screen', '.', '</s>']
The following parameters need to be computed:
". </s>" : 0.5967157691378894
"laughed off" : 6.633939233116624e-05
"the screen" : 0.00010078613182826043
"<s> he" : 0.05200945626477541
"was laughed" : 4.955401387512388e-05
"screen ." : 6.646284726837697e-05
"he was" : 0.0330188679245283
"off the" : 0.004924512408475345

The log base 2 of each of these probabilities is:
". </s>" : -0.7448841920410941
"off the" : -7.665803398813612
"screen ." : -13.877092375027415
"the screen" : -13.276415241892678
"<s> he" : -4.2650822344826524
"laughed off" : -13.879774677336927
"was laughed" : -14.300638553993721
"he was" : -4.920565532505595

The sum of the log probabilities (and the log probability of this text) is:	-72.9302562060937
The average log probability for this text is:	-8.103361800677078
The perplexity for this text is:	275.01410443241207

For the sentence "There was no compulsion behind them . ": 

In the Unigram Model: 
The sentence gets mapped to ['there', 'was', 'no', '<unk>', 'behind', 'them', '.', '</s>']
The parameters required to compute the probabilities are:
"there" : 0.002630832596079361
"was" : 0.010897954173139686
"no" : 0.002112285543754789
"<unk>" : 0.027978259121136825
"behind" : 0.0003513420844321592
"them" : 0.0017186130877042969
"." : 0.047067140202423835
"</s>" : 0.05502948310383217

The log base 2 of each of these probabilities is:
"there" : -8.570264834139998
"was" : -6.519798860395025
"no" : -8.88697940985522
"<unk>" : -5.159549992655863
"behind" : -11.474835983845558
"them" : -9.184539498007306
"." : -4.409135992178209
"</s>" : -4.1836514123893025

The sum of the log probabilities (and the log probability of this text) is:	-58.388755983466474
The average log probability for this text is:	-7.298594497933309
The perplexity for this text is:	157.43303579387805

In the Bigram MLE Model: 
The sentence gets mapped to ['<s>', 'there', 'was', 'no', '<unk>', 'behind', 'them', '.', '</s>']
The following parameters need to be computed:
"was no" : 0.023305496212856866
"no <unk>" : 0.027054108216432865
". </s>" : 1.0
"them ." : 0.1687192118226601
"<unk> behind" : 0.0003782434374763598
"<s> there" : 0.014576923076923078
"there was" : 0.3065164923572003
"behind them" : 0.060240963855421686

The log base 2 of each of these probabilities is:
"<s> there" : -6.100169964632088
". </s>" : 0.0
"there was" : -1.7059633935591632
"<unk> behind" : -11.368397327649257
"no <unk>" : -5.208008503173793
"them ." : -2.5673038342246497
"behind them" : -4.053111336459563
"was no" : -5.423185959188939

The sum of the log probabilities (and the log probability of this text) is:	-36.42614031888746
The average log probability for this text is:	-4.047348924320829
The perplexity for this text is:	16.53382852354821

In the Bigram Add-One Model: 
The sentence gets mapped to ['<s>', 'there', 'was', 'no', '<unk>', 'behind', 'them', '.', '</s>']
The following parameters need to be computed:
"was no" : 0.0059960356788899905
". </s>" : 0.5967157691378894
"no <unk>" : 0.0017468338636221848
"there was" : 0.023473024456187784
"<s> there" : 0.009261290243961882
"<unk> behind" : 0.00021238938053097346
"them ." : 0.008710471501609543
"behind them" : 0.0007238270711324604

The log base 2 of each of these probabilities is:
"them ." : -6.843033470014426
". </s>" : -0.7448841920410941
"there was" : -5.412852447324573
"was no" : -7.381775316719125
"no <unk>" : -9.161041880486328
"behind them" : -10.432067314035452
"<unk> behind" : -12.201000746356119
"<s> there" : -6.754571086976131

The sum of the log probabilities (and the log probability of this text) is:	-58.93122645395324
The average log probability for this text is:	-6.54791405043925
The perplexity for this text is:	93.56610226591661

For the sentence "I look forward to hearing your reply . ": 

In the Unigram Model: 
The sentence gets mapped to ['i', 'look', 'forward', 'to', 'hearing', 'your', 'reply', '.', '</s>']
The parameters required to compute the probabilities are:
"i" : 0.006846937609265272
"look" : 0.0004889157921917396
"forward" : 9.947637330308123e-05
"to" : 0.02071860038859281
"hearing" : 6.349555742749865e-05
"your" : 0.0007767623191964002
"reply" : 6.137903884658203e-05
"." : 0.047067140202423835
"</s>" : 0.05502948310383217

The log base 2 of each of these probabilities is:
"i" : -7.190325418312613
"look" : -10.998126373776424
"forward" : -13.295286563514843
"to" : -5.592929642379461
"hearing" : -13.942984819583963
"your" : -10.33023916234927
"reply" : -13.99189442006491
"." : -4.409135992178209
"</s>" : -4.1836514123893025

The sum of the log probabilities (and the log probability of this text) is:	-83.93457380454899
The average log probability for this text is:	-9.326063756060998
The perplexity for this text is:	641.8372701258673

In the Bigram MLE Model: 
The sentence gets mapped to ['<s>', 'i', 'look', 'forward', 'to', 'hearing', 'your', 'reply', '.', '</s>']
The following parameters need to be computed:
"your reply" : 0.0027247956403269754
"reply ." : 0.20689655172413793
"i look" : 0.0003091190108191654
"to hearing" : 0.0
"forward to" : 0.2765957446808511
"look forward" : 0.017316017316017316
"<s> i" : 0.03523076923076923
". </s>" : 1.0
"hearing your" : 0.0

The log probability is undefined due to the following unseen bigrams:
"to hearing"
"hearing your"

In the Bigram Add-One Model: 
The sentence gets mapped to ['<s>', 'i', 'look', 'forward', 'to', 'hearing', 'your', 'reply', '.', '</s>']
The following parameters need to be computed:
"forward to" : 0.0009285051067780873
". </s>" : 0.5967157691378894
"look forward" : 0.0003276110601493906
"your reply" : 0.000129886998311469
"<s> i" : 0.022348955667665912
"to hearing" : 4.0290088638195005e-05
"hearing your" : 6.639665360865813e-05
"i look" : 0.00010949304719150334
"reply ." : 0.0004648074369189907

The log base 2 of each of these probabilities is:
". </s>" : -0.7448841920410941
"your reply" : -12.910455355309821
"to hearing" : -14.59921549501772
"forward to" : -10.072802534544207
"<s> i" : -5.483648771712023
"reply ." : -11.071079227501686
"hearing your" : -13.878529942862396
"i look" : -13.156873117949567
"look forward" : -11.575728316363783

The sum of the log probabilities (and the log probability of this text) is:	-93.49321695330231
The average log probability for this text is:	-9.34932169533023
The perplexity for this text is:	652.268295379401
